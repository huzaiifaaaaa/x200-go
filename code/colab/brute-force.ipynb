{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, re, struct, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "oVxJ2_wZjhYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/input/20250907-150827_Rtk.fmnav\"  # adjust as needed\n",
        "MAX_RECORDS = 50000\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"‚ùå File not found: {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTMf2F1vj-gs",
        "outputId": "06bbc306-7796-4530-b742-088083199c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"ascii_output.txt\"\n",
        "\n",
        "# =====================================================\n",
        "# 1Ô∏è‚É£ ASCII View ‚Äî readable representation\n",
        "# =====================================================\n",
        "ascii_text = ''.join(chr(b) if 32 <= b < 127 else '.' for b in data)\n",
        "\n",
        "# =====================================================\n",
        "# 2Ô∏è‚É£ Output to both console AND text file\n",
        "# =====================================================\n",
        "chunk_size = 512\n",
        "total_chunks = len(ascii_text) // chunk_size + 1\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
        "\n",
        "    header = (\n",
        "        f\"Read {len(data):,} bytes from file\\n\"\n",
        "        f\"ASCII content in {total_chunks} chunks of {chunk_size} characters:\\n\\n\"\n",
        "    )\n",
        "\n",
        "    print(header)\n",
        "    out.write(header)\n",
        "\n",
        "    for i in range(0, len(ascii_text), chunk_size):\n",
        "        segment = ascii_text[i:i + chunk_size]\n",
        "        line = f\"[{i:06d}-{i+chunk_size:06d}]  {segment}\"\n",
        "\n",
        "        print(line)      # üëà Console\n",
        "        out.write(line + \"\\n\")   # üëà File\n",
        "\n",
        "print(f\"\\n‚úÖ Completed ASCII rendering. Output written to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "jwJ-XfKCiVtB",
        "outputId": "d7f32756-b249-48ea-d344-600db4b1df82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2439844255.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 1Ô∏è‚É£ ASCII View ‚Äî readable representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# =====================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mascii_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m127\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# =====================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ===== HEX DUMP =====\n",
        "\n",
        "# output_hex = \"hex_output.txt\"\n",
        "\n",
        "# with open(file_path, \"rb\") as f:\n",
        "#     data = f.read()\n",
        "\n",
        "# hex_lines = []\n",
        "# bytes_per_line = 16\n",
        "\n",
        "# with open(output_hex, \"w\") as out:\n",
        "#     header = f\"HEX DUMP ‚Äî {len(data):,} bytes\\n\\n\"\n",
        "#     print(header)\n",
        "#     out.write(header)\n",
        "\n",
        "#     for i in range(0, len(data), bytes_per_line):\n",
        "#         chunk = data[i:i+bytes_per_line]\n",
        "#         hex_str = ' '.join(f\"{b:02X}\" for b in chunk)\n",
        "#         line = f\"{i:08X}  {hex_str}\"\n",
        "\n",
        "#         print(line)      # console\n",
        "#         out.write(line + \"\\n\")\n",
        "\n",
        "# print(f\"\\n‚úÖ HEX dump saved to {output_hex}\")\n"
      ],
      "metadata": {
        "id": "pxlbzHxrLXIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HEADER_SKIP = 1024   # based on your ASCII preview\n",
        "payload = data[HEADER_SKIP:]\n",
        "print(f\"üîπ Skipped first {HEADER_SKIP} bytes (header region). Remaining: {len(payload):,} bytes\")\n"
      ],
      "metadata": {
        "id": "I4m5OTQFksGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ascii_view = ''.join(chr(b) if 32 <= b < 127 else '.' for b in payload)\n",
        "nmea_matches = [(m.start(), m.group()) for m in re.finditer(r'\\$G[NPL][A-Z]{3}', ascii_view)]\n",
        "\n",
        "print(f\"üõ∞Ô∏è Found {len(nmea_matches)} NMEA-like sentences:\")\n",
        "for i, (pos, snippet) in enumerate(nmea_matches[:10]):\n",
        "    print(f\"   {i+1:02d}. Offset {pos + HEADER_SKIP:>8,} ‚Üí {snippet}\")\n"
      ],
      "metadata": {
        "id": "JaIZEYiykvlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Decode GNGGA"
      ],
      "metadata": {
        "id": "dhEMWBKXyq6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, struct, numpy as np, pandas as pd\n",
        "import folium\n",
        "\n",
        "# =====================================================\n",
        "# Load file\n",
        "# =====================================================\n",
        "file_path = \"/content/drive/MyDrive/input/20250907-150827_Rtk.fmnav\"\n",
        "\n",
        "with open(file_path, \"rb\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "HEADER_SKIP = 1024\n",
        "payload = data[HEADER_SKIP:]\n",
        "\n",
        "print(f\"üì¶ File loaded ({len(data):,} bytes), payload = {len(payload):,} bytes\")\n",
        "\n",
        "# =====================================================\n",
        "# Step 1 ‚Äî Locate all AA44121C binary frames\n",
        "# =====================================================\n",
        "MAGIC = b\"\\xAA\\x44\\x12\\x1C\"\n",
        "hits = [i for i in range(len(payload)) if payload[i:i+4] == MAGIC]\n",
        "print(f\"üì° Found {len(hits)} AA44121C frames\")"
      ],
      "metadata": {
        "id": "3-nEYMM-3o1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Step 2 ‚Äî Decode GNSS blocks (struct discovered earlier)\n",
        "# =====================================================\n",
        "OFFSET = 12\n",
        "FMT = \"<Qiii fff H\"\n",
        "SIZE = struct.calcsize(FMT)\n",
        "\n",
        "rows = []\n",
        "for h in hits:\n",
        "    block = payload[h+OFFSET : h+OFFSET+SIZE]\n",
        "    if len(block) != SIZE:\n",
        "        continue\n",
        "    try:\n",
        "        rows.append(struct.unpack(FMT, block))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\n",
        "    \"timestamp\", \"lat_i\", \"lon_i\", \"alt_i\",\n",
        "    \"v1\", \"v2\", \"v3\", \"quality\"\n",
        "])\n",
        "\n",
        "print(f\"‚úÖ Successfully decoded {len(df)} GNSS rows\")\n",
        "print(df.head(10))\n",
        "\n",
        "# =====================================================\n",
        "# Step 3 ‚Äî AUTO-CALIBRATE coordinates (important!)\n",
        "# =====================================================\n",
        "\n",
        "# RAW values\n",
        "raw_lat  = df[\"lat_i\"].median()\n",
        "raw_lon  = df[\"lon_i\"].median()\n",
        "\n",
        "# EXPECTED region (Oulu, Finland)\n",
        "expected_lat = 65.06\n",
        "expected_lon = 25.47\n",
        "\n",
        "# Compute scale factors\n",
        "lat_scale = expected_lat / (raw_lat / 1000)\n",
        "lon_scale = expected_lon / (raw_lon / 10000)\n",
        "\n",
        "print(f\"\\nüîß Auto-calibration:\")\n",
        "print(f\"lat_scale = {lat_scale}\")\n",
        "print(f\"lon_scale = {lon_scale}\")\n",
        "\n",
        "# Apply calibrated conversion\n",
        "df[\"lat_deg\"] = (df[\"lat_i\"] / 1000.0) * lat_scale\n",
        "df[\"lon_deg\"] = (df[\"lon_i\"] / 10000.0) * lon_scale\n",
        "df[\"alt_m\"]   = df[\"alt_i\"] * 1.0\n",
        "\n",
        "print(\"\\nüìç Converted GNSS coordinates (first 10):\")\n",
        "print(df[[\"lat_deg\", \"lon_deg\", \"alt_m\"]].head(10))\n",
        "\n",
        "# Filter valid Earth coordinates\n",
        "df_valid = df[\n",
        "    df[\"lat_deg\"].between(40, 80) &\n",
        "    df[\"lon_deg\"].between(0, 40)\n",
        "]\n",
        "\n",
        "print(f\"\\nüìç Valid coordinate points: {len(df_valid)}\")\n",
        "\n",
        "if df_valid.empty:\n",
        "    raise ValueError(\"‚ùå No valid calibrated GNSS points ‚Äî scaling still incorrect.\")\n",
        "\n",
        "# =====================================================\n",
        "# Step 4 ‚Äî Plot on Folium map (Oulu region expected)\n",
        "# =====================================================\n",
        "center_lat = float(df_valid[\"lat_deg\"].mean())\n",
        "center_lon = float(df_valid[\"lon_deg\"].mean())\n",
        "print(f\"üó∫Ô∏è Map center: {center_lat:.6f}, {center_lon:.6f}\")\n",
        "\n",
        "m = folium.Map(location=[center_lat, center_lon], zoom_start=14)\n",
        "\n",
        "# Add GNSS points\n",
        "for _, r in df_valid.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[r[\"lat_deg\"], r[\"lon_deg\"]],\n",
        "        radius=2, color=\"blue\",\n",
        "        fill=True, fill_opacity=0.6\n",
        "    ).add_to(m)\n",
        "\n",
        "# Add center marker\n",
        "folium.Marker(\n",
        "    [center_lat, center_lon],\n",
        "    popup=\"Estimated GNSS center\",\n",
        "    icon=folium.Icon(color=\"red\")\n",
        ").add_to(m)\n",
        "\n",
        "m\n"
      ],
      "metadata": {
        "id": "TWzwer7Ey03A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OLD"
      ],
      "metadata": {
        "id": "tJRC3GdY2WhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window = 200  # number of bytes to check for periodicity\n",
        "step = 4\n",
        "entropy = []"
      ],
      "metadata": {
        "id": "BHYJVoX1lBdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for i in range(0, len(payload) - window, step):\n",
        "    chunk = payload[i:i+window]\n",
        "    unique = len(set(chunk))\n",
        "    entropy.append(unique / window)\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(entropy, color=\"purple\")\n",
        "plt.title(\"Local Byte Uniqueness (proxy for record structure)\")\n",
        "plt.xlabel(\"Byte offset / 4\")\n",
        "plt.ylabel(\"Unique-byte ratio\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ssN5JlRPlAfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ascii_mask = np.array([32 <= b < 127 for b in payload], dtype=np.uint8)\n",
        "block = 120\n",
        "ratio = np.convolve(ascii_mask, np.ones(block, dtype=np.uint8), \"valid\") / block\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(ratio, color=\"darkgreen\")\n",
        "plt.title(\"Printable-character ratio per 120-byte window (binary/text alternation)\")\n",
        "plt.xlabel(\"Byte offset (windowed)\")\n",
        "plt.ylabel(\"ASCII fraction\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sdxx-KiDlGJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_regions = np.where(ratio < 0.05)[0]\n",
        "text_regions = np.where(ratio > 0.2)[0]\n",
        "if len(binary_regions) and len(text_regions):\n",
        "    print(f\"üîç Binary regions start near byte {binary_regions[0]*block + HEADER_SKIP:,}\")\n",
        "    print(f\"üîç First text/NMEA region near byte {text_regions[0]*block + HEADER_SKIP:,}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Could not clearly separate binary vs text windows. Check plots above for visual pattern.\")"
      ],
      "metadata": {
        "id": "VQ7nfdlPlJgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# üß† Step 3 ‚Äî Structured Decoding Attempt (First Binary Block)\n",
        "# =====================================================\n",
        "\n",
        "import struct, re, numpy as np, pandas as pd\n",
        "\n",
        "HEADER_SKIP = 1024  # as confirmed earlier\n",
        "BLOCK_BYTES = 3653  # observed cycle size between NMEA pairs\n",
        "\n",
        "# --- Find NMEA anchors again (within payload) ---\n",
        "ascii_payload = ''.join(chr(b) if 32 <= b < 127 else '.' for b in data[HEADER_SKIP:])\n",
        "nmea_hits = [m.start() for m in re.finditer(r'\\$G[NPL][A-Z]{3}', ascii_payload)]\n",
        "if len(nmea_hits) < 2:\n",
        "    raise ValueError(\"Not enough NMEA sentences detected for segmentation.\")\n",
        "\n",
        "# --- Slice first binary segment (before first NMEA) ---\n",
        "first_bin_end = nmea_hits[0]\n",
        "first_bin_start = max(0, first_bin_end - BLOCK_BYTES)\n",
        "bin_block = data[HEADER_SKIP + first_bin_start : HEADER_SKIP + first_bin_end]\n",
        "print(f\"üß© Binary segment range: {HEADER_SKIP+first_bin_start:,}‚Äì{HEADER_SKIP+first_bin_end:,}  \"\n",
        "      f\"({len(bin_block):,} bytes)\")\n",
        "\n",
        "# --- Candidate record sizes to test ---\n",
        "cand_sizes = [120, 124, 128, 180]\n",
        "print(\"\\nRecord-size divisibility check:\")\n",
        "for sz in cand_sizes:\n",
        "    print(f\"  {sz:>3} bytes ‚Üí remainder {len(bin_block) % sz}\")\n",
        "\n",
        "# --- Candidate formats (common for Feima/STONEX SLAM logs) ---\n",
        "candidates = {\n",
        "    \"A_QdddfffH\": (\"<QdddfffH\", [\"timestamp\",\"lat\",\"lon\",\"alt\",\"vx\",\"vy\",\"vz\",\"quality\"]),\n",
        "    \"B_QfffffH\":  (\"<QfffffH\",  [\"timestamp\",\"x\",\"y\",\"z\",\"extra1\",\"extra2\",\"quality\"]),\n",
        "    \"C_Qdddfff\":  (\"<Qdddfff\",  [\"timestamp\",\"lat\",\"lon\",\"alt\",\"vx\",\"vy\",\"vz\"]),\n",
        "    \"D_Qffffff\":  (\"<Qffffff\",  [\"timestamp\",\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\"]),\n",
        "}\n",
        "\n",
        "# --- Try each layout ---\n",
        "for name, (fmt, headers) in candidates.items():\n",
        "    rec_sz = struct.calcsize(fmt)\n",
        "    nrec = len(bin_block) // rec_sz\n",
        "    if nrec == 0:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüîπ Testing {name}: record {rec_sz} bytes √ó {nrec} records\")\n",
        "    rows = []\n",
        "    for i in range(nrec):\n",
        "        try:\n",
        "            vals = struct.unpack(fmt, bin_block[i*rec_sz:(i+1)*rec_sz])\n",
        "            rows.append(vals)\n",
        "        except struct.error:\n",
        "            break\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=headers)\n",
        "    # quick cleanup: replace invalid with NaN\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    print(f\"Decoded {len(df)} rows\")\n",
        "\n",
        "    # Display quick stats\n",
        "    for c in df.columns:\n",
        "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        if np.isfinite(s).sum() > 0:\n",
        "            print(f\"  {c:<10} ‚Üí min={s.min():.4g}, median={np.nanmedian(s):.4g}, max={s.max():.4g}\")\n",
        "    display(df.head(5))\n"
      ],
      "metadata": {
        "id": "LHXJRtibnaNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Step 4 ‚Äî Decode full 180-byte frames with sync check\n",
        "# =====================================================\n",
        "import numpy as np, pandas as pd, struct\n",
        "\n",
        "REC_SIZE = 180\n",
        "HEADER_SKIP = 1024\n",
        "payload = data[HEADER_SKIP:]\n",
        "nrecs = len(payload) // REC_SIZE\n",
        "print(f\"üì¶ Total records (180 B each): {nrecs}\")\n",
        "\n",
        "fmt_180 = \"<H H Q ddd ddd fff fff fff fff H B 7s\"  # extended Feima pattern\n",
        "headers_180 = [\n",
        "    \"sync\",\"pkt_id\",\"timestamp\",\n",
        "    \"lat\",\"lon\",\"alt\",\n",
        "    \"vx\",\"vy\",\"vz\",\n",
        "    \"ax\",\"ay\",\"az\",\n",
        "    \"gx\",\"gy\",\"gz\",\n",
        "    \"roll\",\"pitch\",\"yaw\",\n",
        "    \"sats\",\"fix_type\",\"pad\"\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for i in range(nrecs):\n",
        "    chunk = payload[i*REC_SIZE:(i+1)*REC_SIZE]\n",
        "    try:\n",
        "        vals = struct.unpack(fmt_180, chunk)\n",
        "        rows.append(vals)\n",
        "    except struct.error:\n",
        "        continue\n",
        "\n",
        "df = pd.DataFrame(rows, columns=headers_180)\n",
        "\n",
        "# --- Filter valid syncs (0xAA55 or 0x55AA) ---\n",
        "df = df[df[\"sync\"].isin([0xAA55, 0x55AA])]\n",
        "\n",
        "# --- Quick numeric cleanup ---\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df = df.fillna(0)\n",
        "\n",
        "# --- Display sample ---\n",
        "print(f\"‚úÖ Decoded {len(df)} valid frames ({len(df)*REC_SIZE:,} bytes)\\n\")\n",
        "display(df.head(5))\n",
        "\n",
        "# --- Basic sanity check ---\n",
        "for col in [\"lat\",\"lon\",\"alt\",\"vx\",\"vy\",\"vz\",\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\"]:\n",
        "    s = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    print(f\"{col:>5}: min={s.min():.4g}, med={np.nanmedian(s):.4g}, max={s.max():.4g}\")\n"
      ],
      "metadata": {
        "id": "QH_r0_a5n-uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Step 5 ‚Äî Find true sync / magic header pattern\n",
        "# =====================================================\n",
        "import struct, collections\n",
        "\n",
        "REC_SIZE = 180\n",
        "HEADER_SKIP = 1024\n",
        "payload = data[HEADER_SKIP:]\n",
        "nrecs = len(payload) // REC_SIZE\n",
        "print(f\"Scanning {nrecs} √ó {REC_SIZE}-byte frames...\")\n",
        "\n",
        "# --- Count 2-byte patterns at frame starts and nearby offsets\n",
        "freq_2 = collections.Counter()\n",
        "freq_4 = collections.Counter()\n",
        "\n",
        "for i in range(nrecs):\n",
        "    start = i * REC_SIZE\n",
        "    chunk = payload[start:start+16]  # first 16 bytes of each frame\n",
        "    for off in range(0, 8, 2):  # check first few offsets\n",
        "        if off+2 <= len(chunk):\n",
        "            key2 = chunk[off:off+2]\n",
        "            freq_2[key2] += 1\n",
        "        if off+4 <= len(chunk):\n",
        "            key4 = chunk[off:off+4]\n",
        "            freq_4[key4] += 1\n",
        "\n",
        "def show_top(counter, label, n=10):\n",
        "    print(f\"\\nTop {n} {label} patterns:\")\n",
        "    for val, cnt in counter.most_common(n):\n",
        "        print(f\"  {val.hex().upper():<12}  {cnt:>6} √ó\")\n",
        "\n",
        "show_top(freq_2, \"2-byte\")\n",
        "show_top(freq_4, \"4-byte\")\n",
        "\n",
        "# --- Optional: look for high-frequency printable ASCII sequences (helpful for header markers)\n",
        "ascii_freq = collections.Counter()\n",
        "for i in range(0, len(payload)-4, 180):\n",
        "    chunk = payload[i:i+8]\n",
        "    if all(32 <= b < 127 for b in chunk):\n",
        "        ascii_freq[chunk] += 1\n",
        "show_top(ascii_freq, \"ASCII\", 5)\n"
      ],
      "metadata": {
        "id": "BC_hM4_5oP-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Step 6 ‚Äî Locate and decode frames starting with AA44121C\n",
        "# =====================================================\n",
        "import struct, numpy as np, pandas as pd\n",
        "\n",
        "MAGIC = b\"\\xAA\\x44\\x12\\x1C\"\n",
        "REC_SIZE = 180\n",
        "HEADER_SKIP = 1024\n",
        "payload = data[HEADER_SKIP:]\n",
        "hits = [i for i in range(0, len(payload)-4) if payload[i:i+4] == MAGIC]\n",
        "\n",
        "print(f\"Found {len(hits)} frame headers with AA44121C\")\n",
        "\n",
        "# Show spacing to verify periodicity\n",
        "if len(hits) > 1:\n",
        "    diffs = np.diff(hits)\n",
        "    print(\"Most common spacing between headers:\", pd.Series(diffs).value_counts().head())\n",
        "\n",
        "# --- Extract clean frames\n",
        "records = []\n",
        "for h in hits:\n",
        "    chunk = payload[h:h+REC_SIZE]\n",
        "    if len(chunk) == REC_SIZE:\n",
        "        records.append(chunk)\n",
        "print(f\"Extracted {len(records)} √ó {REC_SIZE}-byte frames ({len(records)*REC_SIZE:,} bytes)\")\n",
        "\n",
        "# --- Try dynamic decoding (mix of double/float/int)\n",
        "fmt_guess = \"<Q ddd fff fff H\"\n",
        "headers = [\"timestamp\",\"lat\",\"lon\",\"alt\",\"vx\",\"vy\",\"vz\",\"sats\"]\n",
        "rec_sz = struct.calcsize(fmt_guess)\n",
        "decoded = []\n",
        "\n",
        "for rec in records:\n",
        "    try:\n",
        "        vals = struct.unpack(fmt_guess, rec[:rec_sz])\n",
        "        decoded.append(vals)\n",
        "    except struct.error:\n",
        "        continue\n",
        "\n",
        "df = pd.DataFrame(decoded, columns=headers)\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "print(f\"‚úÖ Decoded {len(df)} candidate GNSS frames\")\n",
        "display(df.head(10))\n",
        "\n",
        "# --- Simple summary to assess realism\n",
        "for c in [\"lat\",\"lon\",\"alt\",\"vx\",\"vy\",\"vz\"]:\n",
        "    s = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    if np.isfinite(s).sum() > 0:\n",
        "        print(f\"{c:<4}: min={s.min():.6g}, med={np.nanmedian(s):.6g}, max={s.max():.6g}\")\n"
      ],
      "metadata": {
        "id": "s8kVNPd3omxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Step 7 ‚Äî Dynamic struct analysis for AA44121C frames\n",
        "# =====================================================\n",
        "import struct, pandas as pd, numpy as np\n",
        "\n",
        "MAGIC = b\"\\xAA\\x44\\x12\\x1C\"\n",
        "payload = data[1024:]\n",
        "hits = [i for i in range(0, len(payload)-4) if payload[i:i+4] == MAGIC]\n",
        "\n",
        "print(f\"Found {len(hits)} AA44121C headers\")\n",
        "\n",
        "# Candidate struct patterns to test (growing complexity)\n",
        "candidates = {\n",
        "    \"A_<QdddfffH\": \"<QdddfffH\",\n",
        "    \"B_<QdddffffH\": \"<QdddffffH\",\n",
        "    \"C_<QdddfffffH\": \"<QdddfffffH\",\n",
        "    \"D_<QdddffffffH\": \"<QdddffffffH\",\n",
        "}\n",
        "\n",
        "records = []\n",
        "for h in hits:\n",
        "    chunk = payload[h:h+180]\n",
        "    if len(chunk) == 180:\n",
        "        records.append(chunk)\n",
        "print(f\"Collected {len(records)} 180B chunks\")\n",
        "\n",
        "# Try decoding with multiple candidates\n",
        "results = []\n",
        "for name, fmt in candidates.items():\n",
        "    size = struct.calcsize(fmt)\n",
        "    valid_rows = []\n",
        "    for rec in records[:500]:  # sample only a few hundred to test\n",
        "        try:\n",
        "            vals = struct.unpack(fmt, rec[:size])\n",
        "            valid_rows.append(vals)\n",
        "        except struct.error:\n",
        "            pass\n",
        "    if valid_rows:\n",
        "        print(f\"\\nüîπ {name}: unpacked {len(valid_rows)} rows ({len(valid_rows[0])} cols), size={size}B\")\n",
        "        df = pd.DataFrame(valid_rows)\n",
        "        print(df.head(5))\n",
        "        results.append((name, df))\n",
        "\n",
        "# Suggest best candidate\n",
        "if results:\n",
        "    longest = max(results, key=lambda x: x[1].shape[1])\n",
        "    print(f\"\\n‚úÖ Best candidate appears to be {longest[0]} with {longest[1].shape[1]} columns.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No pattern decoded successfully.\")\n"
      ],
      "metadata": {
        "id": "Xc5rF1piozXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Step 9 ‚Äî Hybrid (int/float) sweep after MAGIC; rank by plausibility and show best\n",
        "# =====================================================\n",
        "import struct, re, numpy as np, pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "HEADER_SKIP = 1024\n",
        "REC_SIZE = 180\n",
        "MAGIC = b\"\\xAA\\x44\\x12\\x1C\"\n",
        "\n",
        "payload = data[HEADER_SKIP:]\n",
        "# Locate frame starts at MAGIC; slice 180B per frame\n",
        "hits = [i for i in range(0, len(payload)-4) if payload[i:i+4] == MAGIC]\n",
        "records = [payload[h:h+REC_SIZE] for h in hits if h+REC_SIZE <= len(payload)]\n",
        "print(f\"üß© Frames collected from MAGIC: {len(records)} √ó {REC_SIZE} B\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def try_unpack(rec, start, fmt):\n",
        "    size = struct.calcsize(fmt)\n",
        "    if start + size > len(rec):\n",
        "        return None\n",
        "    try:\n",
        "        return struct.unpack(fmt, rec[start:start+size])\n",
        "    except struct.error:\n",
        "        return None\n",
        "\n",
        "def timestamp_score(ts):\n",
        "    s = pd.Series(pd.to_numeric(ts, errors=\"coerce\"), dtype=\"float64\")\n",
        "    s = s[np.isfinite(s)]\n",
        "    if len(s) < 8: return -1.0\n",
        "    d = np.diff(s)\n",
        "    frac_pos = (d > 0).sum() / max(1, len(d))\n",
        "    span = s.max() - s.min()\n",
        "    span_score = 0.0 if span <= 0 else min(1.0, math.log10(span + 1e-9)/12.0)\n",
        "    return 0.6*frac_pos + 0.4*span_score\n",
        "\n",
        "def range_fraction(x, lo, hi):\n",
        "    x = pd.to_numeric(x, errors=\"coerce\")\n",
        "    x = x[np.isfinite(x)]\n",
        "    if len(x) == 0: return 0.0\n",
        "    return ((x >= lo) & (x <= hi)).mean()\n",
        "\n",
        "def accel_score(ax, ay, az):\n",
        "    # prefer magnitudes around gravity with reasonable spread\n",
        "    ax = pd.to_numeric(ax, errors=\"coerce\"); ay = pd.to_numeric(ay, errors=\"coerce\"); az = pd.to_numeric(az, errors=\"coerce\")\n",
        "    m = np.sqrt(ax**2 + ay**2 + az**2)\n",
        "    m = m[np.isfinite(m)]\n",
        "    if len(m) == 0: return 0.0\n",
        "    med = float(np.nanmedian(m))\n",
        "    # score peaks near 9.8, allow broad tolerance\n",
        "    return max(0.0, 1.0 - abs(med - 9.8)/9.8)\n",
        "\n",
        "def clip_df(df):\n",
        "    # avoid NaNs/Infs wrecking plots\n",
        "    return df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# ---------- candidate schemas ----------\n",
        "# We start decoding AFTER the 4B MAGIC; also try small extra offsets (e.g., 2B/4B packet id)\n",
        "relative_starts = [4, 6, 8, 10, 12]\n",
        "\n",
        "schemas = [\n",
        "    # name, fmt, headers, type-hints for scaling ints (None/ 'deg1e7' / 'milli')\n",
        "    (\"Qiii_fff_H\",   \"<Qiii fff H\",   [\"timestamp\",\"lat_i\",\"lon_i\",\"alt_i\",\"v1\",\"v2\",\"v3\",\"q\"],       {\"lat_i\":\"deg1e7\",\"lon_i\":\"deg1e7\",\"alt_i\":\"milli\"}),\n",
        "    (\"Qiiifff_H\",    \"<Qiiifff H\",    [\"timestamp\",\"x_i\",\"y_i\",\"z_i\",\"v1\",\"v2\",\"v3\",\"q\"],            {\"x_i\":\"milli\",\"y_i\":\"milli\",\"z_i\":\"milli\"}),\n",
        "    (\"Qfff_fff_H\",   \"<Qfff fff H\",   [\"timestamp\",\"x\",\"y\",\"z\",\"v1\",\"v2\",\"v3\",\"q\"],                  {}),\n",
        "    (\"Qiii_hhh_H\",   \"<Qiii hhh H\",   [\"timestamp\",\"lat_i\",\"lon_i\",\"alt_i\",\"ax_i16\",\"ay_i16\",\"az_i16\",\"q\"],\n",
        "                                                                                                      {\"lat_i\":\"deg1e7\",\"lon_i\":\"deg1e7\",\"alt_i\":\"milli\",\"ax_i16\":\"g16\",\"ay_i16\":\"g16\",\"az_i16\":\"g16\"}),\n",
        "    (\"Qfff_hhh_H\",   \"<Qfff hhh H\",   [\"timestamp\",\"x\",\"y\",\"z\",\"ax_i16\",\"ay_i16\",\"az_i16\",\"q\"],      {\"ax_i16\":\"g16\",\"ay_i16\":\"g16\",\"az_i16\":\"g16\"}),\n",
        "    (\"Qiiiiii_H\",    \"<Qiiiiii H\",    [\"timestamp\",\"a_i\",\"b_i\",\"c_i\",\"d_i\",\"e_i\",\"f_i\",\"q\"],         {\"a_i\":\"milli\",\"b_i\":\"milli\",\"c_i\":\"milli\",\"d_i\":\"milli\",\"e_i\":\"milli\",\"f_i\":\"milli\"}),\n",
        "]\n",
        "\n",
        "def apply_scaling(df, hints):\n",
        "    out = df.copy()\n",
        "    for col, kind in (hints or {}).items():\n",
        "        if col not in out: continue\n",
        "        if kind == \"deg1e7\":\n",
        "            out[col] = pd.to_numeric(out[col], errors=\"coerce\") * 1e-7\n",
        "        elif kind == \"milli\":\n",
        "            out[col] = pd.to_numeric(out[col], errors=\"coerce\") * 1e-3\n",
        "        elif kind == \"g16\":\n",
        "            # assume int16 counts where 16384 ‚âà 1 g\n",
        "            out[col] = pd.to_numeric(out[col], errors=\"coerce\") / 16384.0 * 9.80665\n",
        "    return out\n",
        "\n",
        "def score_decode(df):\n",
        "    sc = 0.0\n",
        "    if \"timestamp\" in df:\n",
        "        sc += 1.5 * timestamp_score(df[\"timestamp\"])\n",
        "    # prefer plausible geodetic if present\n",
        "    if {\"lat_i\",\"lon_i\"}.issubset(df.columns):\n",
        "        sc += 1.0 * range_fraction(df[\"lat_i\"], -90, 90)\n",
        "        sc += 1.0 * range_fraction(df[\"lon_i\"], -180, 180)\n",
        "    if {\"x\",\"y\"}.issubset(df.columns):\n",
        "        # assume ENU meters range sanity (within +/- 1e6)\n",
        "        sc += 0.6 * range_fraction(df[\"x\"], -1e6, 1e6)\n",
        "        sc += 0.6 * range_fraction(df[\"y\"], -1e6, 1e6)\n",
        "    if \"alt_i\" in df:\n",
        "        sc += 0.8 * range_fraction(df[\"alt_i\"], -500, 10000)\n",
        "    if {\"v1\",\"v2\",\"v3\"}.issubset(df.columns):\n",
        "        vmag = np.sqrt(pd.to_numeric(df[\"v1\"], errors=\"coerce\")**2 +\n",
        "                       pd.to_numeric(df[\"v2\"], errors=\"coerce\")**2 +\n",
        "                       pd.to_numeric(df[\"v3\"], errors=\"coerce\")**2)\n",
        "        vmag = vmag[np.isfinite(vmag)]\n",
        "        if len(vmag):\n",
        "            sc += 0.6 * ( (vmag < 100).mean() )  # prefer <100 m/s\n",
        "    # accel closeness to g if any\n",
        "    if {\"ax_i16\",\"ay_i16\",\"az_i16\"}.issubset(df.columns):\n",
        "        sc += 0.8 * accel_score(df[\"ax_i16\"], df[\"ay_i16\"], df[\"az_i16\"])\n",
        "    return sc\n",
        "\n",
        "# ---------- sweep ----------\n",
        "results = []\n",
        "sample_n = min(1500, len(records))  # limit for speed\n",
        "for rel_start in relative_starts:\n",
        "    for name, fmt, headers, hints in schemas:\n",
        "        rows = []\n",
        "        size = struct.calcsize(fmt)\n",
        "        for rec in records[:sample_n]:\n",
        "            vals = try_unpack(rec, rel_start, fmt)\n",
        "            if vals is not None:\n",
        "                rows.append(vals)\n",
        "        if len(rows) == 0:\n",
        "            continue\n",
        "        df = pd.DataFrame(rows, columns=headers)\n",
        "        df = apply_scaling(df, hints)\n",
        "        df = clip_df(df)\n",
        "        sc = score_decode(df)\n",
        "        results.append((sc, rel_start, name, fmt, headers, hints, df))\n",
        "\n",
        "# rank and show top 3\n",
        "if not results:\n",
        "    raise RuntimeError(\"No hybrid schema produced rows.\")\n",
        "\n",
        "results.sort(key=lambda x: x[0], reverse=True)\n",
        "topk = results[:3]\n",
        "print(\"üèÅ Top candidates (score, start offset, schema):\")\n",
        "for sc, rel_start, name, fmt, headers, hints, df in topk:\n",
        "    print(f\"  score={sc:6.3f}  start=+{rel_start:2d}  {name:<12}  fmt={fmt}  rows={len(df)}\")\n",
        "\n",
        "# show best\n",
        "best_sc, best_off, best_name, best_fmt, best_headers, best_hints, best_df = topk[0]\n",
        "print(f\"\\n‚úÖ BEST ‚Üí score={best_sc:.3f}, start=+{best_off}, schema={best_name}, fmt={best_fmt}\")\n",
        "display(best_df.head(25))\n",
        "\n",
        "# quick visuals if geodetic present\n",
        "if {\"lat_i\",\"lon_i\"}.issubset(best_df.columns):\n",
        "    lat = pd.to_numeric(best_df[\"lat_i\"], errors=\"coerce\")\n",
        "    lon = pd.to_numeric(best_df[\"lon_i\"], errors=\"coerce\")\n",
        "    m = np.isfinite(lat) & np.isfinite(lon)\n",
        "    if m.sum() > 0:\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.scatter(lon[m][::max(1,len(lon)//5000)], lat[m][::max(1,len(lat)//5000)], s=2, alpha=0.5)\n",
        "        plt.xlabel(\"lon\"); plt.ylabel(\"lat\"); plt.title(\"Lat/Lon scatter (best candidate)\")\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "# print key ranges\n",
        "def stats(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df:\n",
        "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "            s = s[np.isfinite(s)]\n",
        "            if len(s):\n",
        "                print(f\"{c:>8}: min={s.min():.6g}  med={np.nanmedian(s):.6g}  max={s.max():.6g}\")\n",
        "\n",
        "print(\"\\nüìä Ranges (best candidate):\")\n",
        "stats(best_df, [\"timestamp\",\"lat_i\",\"lon_i\",\"alt_i\",\"x\",\"y\",\"z\",\"v1\",\"v2\",\"v3\",\"ax_i16\",\"ay_i16\",\"az_i16\",\"q\"])\n"
      ],
      "metadata": {
        "id": "aKfcPdL0sSD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Step 10 ‚Äî Decode ALL frames with the confirmed schema and visualize\n",
        "#   Best from Step 9: start offset = +12, fmt = \"<Qiii fff H\"\n",
        "#   We export BOTH interpretations:\n",
        "#     (A) Geodetic scaling:  int32 ‚Üí degrees/meters via √ó1e-7 and √ó1e-3\n",
        "#     (B) ENU/meters scaling: int32 ‚Üí meters via √ó1e-3\n",
        "# =====================================================\n",
        "import struct, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "HEADER_SKIP = 1024\n",
        "REC_SIZE    = 180\n",
        "MAGIC       = b\"\\xAA\\x44\\x12\\x1C\"\n",
        "START_OFF   = 12           # bytes from frame start (MAGIC at offset 0)\n",
        "FMT         = \"<Qiii fff H\"  # timestamp, 3√óint32, 3√ófloat32, uint16\n",
        "COLS        = [\"timestamp\",\"i1\",\"i2\",\"i3\",\"v1\",\"v2\",\"v3\",\"q\"]\n",
        "\n",
        "payload = data[HEADER_SKIP:]\n",
        "\n",
        "# ---- Collect frames that start at MAGIC and have full 180B\n",
        "hits = [i for i in range(0, len(payload)-4) if payload[i:i+4] == MAGIC]\n",
        "frames = [payload[h:h+REC_SIZE] for h in hits if h + REC_SIZE <= len(payload)]\n",
        "print(f\"üì¶ Frames (MAGIC-aligned @180B): {len(frames)}\")\n",
        "\n",
        "# ---- Unpack using the confirmed schema at start offset +12\n",
        "rec_size = struct.calcsize(FMT)\n",
        "rows = []\n",
        "for fr in frames:\n",
        "    if START_OFF + rec_size <= len(fr):\n",
        "        try:\n",
        "            vals = struct.unpack(FMT, fr[START_OFF:START_OFF+rec_size])\n",
        "            rows.append(vals)\n",
        "        except struct.error:\n",
        "            rows.append(None)\n",
        "\n",
        "rows = [r for r in rows if r is not None]\n",
        "df = pd.DataFrame(rows, columns=COLS)\n",
        "print(f\"‚úÖ Decoded rows: {len(df)}\")\n",
        "\n",
        "# ---- Create BOTH interpretations\n",
        "# Geodetic-style scaling (common in GNSS logs):\n",
        "#   int32 * 1e-7 ‚Üí degrees,  int32 * 1e-3 ‚Üí meters\n",
        "df[\"lat_deg\"] = df[\"i1\"] * 1e-7\n",
        "df[\"lon_deg\"] = df[\"i2\"] * 1e-7\n",
        "df[\"alt_m\"]   = df[\"i3\"] * 1e-3\n",
        "\n",
        "# ENU/meters-style (if the ints are local metric coordinates):\n",
        "df[\"x_m\"] = df[\"i1\"] * 1e-3\n",
        "df[\"y_m\"] = df[\"i2\"] * 1e-3\n",
        "df[\"z_m\"] = df[\"i3\"] * 1e-3\n",
        "\n",
        "# ---- Show head for quick inspection\n",
        "display(df[[\"timestamp\",\"lat_deg\",\"lon_deg\",\"alt_m\",\"v1\",\"v2\",\"v3\",\"q\",\"x_m\",\"y_m\",\"z_m\"]].head(25))\n",
        "\n",
        "# ---- Basic stats\n",
        "def stats(s):\n",
        "    s = pd.to_numeric(s, errors=\"coerce\")\n",
        "    s = s[np.isfinite(s)]\n",
        "    if len(s)==0: return \"n/a\"\n",
        "    return f\"min={s.min():.6g}, med={np.nanmedian(s):.6g}, max={s.max():.6g}\"\n",
        "\n",
        "print(\"\\nüìä RANGES\")\n",
        "for c in [\"lat_deg\",\"lon_deg\",\"alt_m\",\"v1\",\"v2\",\"v3\",\"x_m\",\"y_m\",\"z_m\"]:\n",
        "    print(f\"{c:>8}: {stats(df[c])}\")\n",
        "\n",
        "# ---- Visualizations\n",
        "plt.figure(figsize=(6,3))\n",
        "ts = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\")\n",
        "dts = np.diff(ts.values.astype(np.float64))\n",
        "plt.plot(dts[:2000])\n",
        "plt.title(\"Timestamp deltas (first ~2000)\")\n",
        "plt.xlabel(\"record\"); plt.ylabel(\"Œî timestamp\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Decide which 2D scatter to show:\n",
        "# If geodetic looks plausible (‚â•50% points in valid range), plot lat/lon; else plot x/y (meters)\n",
        "valid_lat = df[\"lat_deg\"].between(-90, 90)\n",
        "valid_lon = df[\"lon_deg\"].between(-180, 180)\n",
        "geodetic_ok = (valid_lat & valid_lon).mean() >= 0.5\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "if geodetic_ok:\n",
        "    m = (valid_lat & valid_lon)\n",
        "    # decimate for plotting\n",
        "    step = max(1, m.sum()//8000)\n",
        "    plt.scatter(df.loc[m, \"lon_deg\"][::step], df.loc[m, \"lat_deg\"][::step], s=2, alpha=0.5)\n",
        "    plt.xlabel(\"lon (deg)\"); plt.ylabel(\"lat (deg)\")\n",
        "    plt.title(\"Lat/Lon scatter (scaled int32 √ó1e-7)\")\n",
        "else:\n",
        "    # fall back to ENU meters-style view\n",
        "    xm = pd.to_numeric(df[\"x_m\"], errors=\"coerce\")\n",
        "    ym = pd.to_numeric(df[\"y_m\"], errors=\"coerce\")\n",
        "    m = np.isfinite(xm) & np.isfinite(ym)\n",
        "    step = max(1, m.sum()//8000)\n",
        "    plt.scatter(xm[m][::step], ym[m][::step], s=2, alpha=0.5)\n",
        "    plt.xlabel(\"x (m)\"); plt.ylabel(\"y (m)\")\n",
        "    plt.title(\"Planar scatter (assuming int32 √ó1e-3 m)\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# ---- Optional: save CSV in Colab workspace (not Drive)\n",
        "out_path = \"/content/x200go_gnss_decoded.csv\"\n",
        "df_out = df[[\"timestamp\",\"lat_deg\",\"lon_deg\",\"alt_m\",\"v1\",\"v2\",\"v3\",\"q\",\"x_m\",\"y_m\",\"z_m\"]].copy()\n",
        "df_out.to_csv(out_path, index=False)\n",
        "print(f\"üíæ Saved CSV to {out_path}\")\n"
      ],
      "metadata": {
        "id": "4ewOfHWGspkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Step 11 ‚Äî Extract IMU subpacket from each 180B frame\n",
        "#   GNSS subpacket confirmed at start+12 with <Qiii fff H> (34 B)\n",
        "#   IMU likely lives later in the same 180B frame.\n",
        "#   We sweep offsets and formats to find plausible accel/gyro.\n",
        "# =====================================================\n",
        "import struct, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "HEADER_SKIP = 1024\n",
        "REC_SIZE    = 180\n",
        "MAGIC       = b\"\\xAA\\x44\\x12\\x1C\"\n",
        "\n",
        "GNSS_OFF    = 12\n",
        "GNSS_FMT    = \"<Qiii fff H\"\n",
        "GNSS_SIZE   = struct.calcsize(GNSS_FMT)  # 34 bytes\n",
        "\n",
        "payload = data[HEADER_SKIP:]\n",
        "hits = [i for i in range(0, len(payload)-4) if payload[i:i+4] == MAGIC]\n",
        "frames = [payload[h:h+REC_SIZE] for h in hits if h + REC_SIZE <= len(payload)]\n",
        "print(f\"Frames (MAGIC-aligned @180B): {len(frames)}\")\n",
        "\n",
        "# ---- Decode GNSS for reference (also gives usable timestamps)\n",
        "gnss_rows = []\n",
        "for fr in frames:\n",
        "    try:\n",
        "        vals = struct.unpack(GNSS_FMT, fr[GNSS_OFF:GNSS_OFF+GNSS_SIZE])\n",
        "        gnss_rows.append(vals)\n",
        "    except struct.error:\n",
        "        gnss_rows.append(None)\n",
        "\n",
        "gnss_rows = [r for r in gnss_rows if r is not None]\n",
        "gnss = pd.DataFrame(gnss_rows, columns=[\"timestamp\",\"i1\",\"i2\",\"i3\",\"v1\",\"v2\",\"v3\",\"q\"])\n",
        "# Meter interpretation (seems most plausible for these ints)\n",
        "gnss[\"x_m\"] = gnss[\"i1\"] * 1e-3\n",
        "gnss[\"y_m\"] = gnss[\"i2\"] * 1e-3\n",
        "gnss[\"z_m\"] = gnss[\"i3\"] * 1e-3\n",
        "\n",
        "# ---- Define IMU candidate formats\n",
        "# Common encodings:\n",
        "#  - int16 accel/gyro (¬±16 g and ¬±2000 dps ranges), often scaled\n",
        "#  - float32 accel/gyro\n",
        "#  - sometimes two triplets accel+gyro back-to-back\n",
        "imu_candidates = [\n",
        "    # name, fmt, headers, scaling dict\n",
        "    (\"h6\",    \"<hhhhhh\", [\"ax_i16\",\"ay_i16\",\"az_i16\",\"gx_i16\",\"gy_i16\",\"gz_i16\"],\n",
        "              {\"ax_i16\":\"g16\",\"ay_i16\":\"g16\",\"az_i16\":\"g16\",\"gx_i16\":\"dps16\",\"gy_i16\":\"dps16\",\"gz_i16\":\"dps16\"}),\n",
        "    (\"f6\",    \"<ffffff\", [\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\"], {}),\n",
        "    (\"h3f3\",  \"<hhhfff\", [\"ax_i16\",\"ay_i16\",\"az_i16\",\"gx\",\"gy\",\"gz\"],\n",
        "              {\"ax_i16\":\"g16\",\"ay_i16\":\"g16\",\"az_i16\":\"g16\"}),\n",
        "    (\"f3h3\",  \"<fffhhh\", [\"ax\",\"ay\",\"az\",\"gx_i16\",\"gy_i16\",\"gz_i16\"],\n",
        "              {\"gx_i16\":\"dps16\",\"gy_i16\":\"dps16\",\"gz_i16\":\"dps16\"}),\n",
        "    # two triplets accel+gyro repeated (12 fields total)\n",
        "    (\"h12\",   \"<hhhhhhhhhhhh\",\n",
        "              [\"a1\",\"a2\",\"a3\",\"g1\",\"g2\",\"g3\",\"a4\",\"a5\",\"a6\",\"g4\",\"g5\",\"g6\"],\n",
        "              {\"a1\":\"g16\",\"a2\":\"g16\",\"a3\":\"g16\",\"a4\":\"g16\",\"a5\":\"g16\",\"a6\":\"g16\",\n",
        "               \"g1\":\"dps16\",\"g2\":\"dps16\",\"g3\":\"dps16\",\"g4\":\"dps16\",\"g5\":\"dps16\",\"g6\":\"dps16\"}),\n",
        "]\n",
        "\n",
        "# plausible IMU region starts: just after GNSS subpacket, with small paddings\n",
        "IMU_STARTS = [GNSS_OFF + GNSS_SIZE + d for d in (0, 2, 4, 6, 8, 10, 12, 16, 20, 24)]\n",
        "\n",
        "def apply_scaling(df, scale_map):\n",
        "    out = df.copy()\n",
        "    for col, kind in (scale_map or {}).items():\n",
        "        if col not in out: continue\n",
        "        s = pd.to_numeric(out[col], errors=\"coerce\")\n",
        "        if kind == \"g16\":\n",
        "            # assume 16384 counts = 1 g\n",
        "            out[col] = s / 16384.0 * 9.80665\n",
        "        elif kind == \"dps16\":\n",
        "            # assume 16.4 cts/¬∞/s (MPU-6000 style); adjust if needed\n",
        "            out[col] = s / 16.4\n",
        "    return out\n",
        "\n",
        "def accel_score(ax, ay, az):\n",
        "    ax = pd.to_numeric(ax, errors=\"coerce\")\n",
        "    ay = pd.to_numeric(ay, errors=\"coerce\")\n",
        "    az = pd.to_numeric(az, errors=\"coerce\")\n",
        "    m = np.sqrt(ax**2 + ay**2 + az**2)\n",
        "    m = m[np.isfinite(m)]\n",
        "    if len(m) == 0: return 0.0\n",
        "    med = float(np.nanmedian(m))\n",
        "    # score peaks near 1 g\n",
        "    return max(0.0, 1.0 - abs(med - 9.80665)/9.80665)\n",
        "\n",
        "def gyro_score(gx, gy, gz):\n",
        "    gx = pd.to_numeric(gx, errors=\"coerce\"); gy = pd.to_numeric(gy, errors=\"coerce\"); gz = pd.to_numeric(gz, errors=\"coerce\")\n",
        "    m = np.sqrt(gx**2 + gy**2 + gz**2); m = m[np.isfinite(m)]\n",
        "    if len(m) == 0: return 0.0\n",
        "    # prefer med < 500 dps, not all zeros\n",
        "    med = float(np.nanmedian(m))\n",
        "    zfrac = (m == 0).mean()\n",
        "    return max(0.0, 1.0 - med/500.0) * (1.0 - zfrac)\n",
        "\n",
        "def try_imu_at(start, fmt, headers, scale_map):\n",
        "    size = struct.calcsize(fmt)\n",
        "    rows = []\n",
        "    for fr in frames:\n",
        "        if start + size <= len(fr):\n",
        "            try:\n",
        "                rows.append(struct.unpack(fmt, fr[start:start+size]))\n",
        "            except struct.error:\n",
        "                rows.append(None)\n",
        "        else:\n",
        "            rows.append(None)\n",
        "    rows = [r for r in rows if r is not None]\n",
        "    if not rows: return None\n",
        "    df = pd.DataFrame(rows, columns=headers)\n",
        "    df = apply_scaling(df, scale_map)\n",
        "    return df\n",
        "\n",
        "# Sweep candidates and rank\n",
        "results = []\n",
        "for st in IMU_STARTS:\n",
        "    for name, fmt, headers, scale_map in imu_candidates:\n",
        "        df_imu = try_imu_at(st, fmt, headers, scale_map)\n",
        "        if df_imu is None or len(df_imu) < 50:\n",
        "            continue\n",
        "        # compute scores\n",
        "        a_sc = 0.0; g_sc = 0.0\n",
        "        if set([\"ax\",\"ay\",\"az\"]).issubset(df_imu.columns):\n",
        "            a_sc = accel_score(df_imu[\"ax\"], df_imu[\"ay\"], df_imu[\"az\"])\n",
        "        elif set([\"ax_i16\",\"ay_i16\",\"az_i16\"]).issubset(df_imu.columns):\n",
        "            a_sc = accel_score(df_imu[\"ax_i16\"], df_imu[\"ay_i16\"], df_imu[\"az_i16\"])\n",
        "        if set([\"gx\",\"gy\",\"gz\"]).issubset(df_imu.columns):\n",
        "            g_sc = gyro_score(df_imu[\"gx\"], df_imu[\"gy\"], df_imu[\"gz\"])\n",
        "        elif set([\"gx_i16\",\"gy_i16\",\"gz_i16\"]).issubset(df_imu.columns):\n",
        "            g_sc = gyro_score(df_imu[\"gx_i16\"], df_imu[\"gy_i16\"], df_imu[\"gz_i16\"])\n",
        "        score = a_sc + g_sc\n",
        "        results.append((score, st, name, fmt, headers, df_imu))\n",
        "\n",
        "# Show top hits\n",
        "results.sort(key=lambda x: x[0], reverse=True)\n",
        "print(\"\\nTop IMU candidates (score = accel‚âà1g + gyro<500dps):\")\n",
        "for score, st, name, fmt, headers, df_imu in results[:5]:\n",
        "    print(f\"  score={score:5.3f}  start=+{st:3d}  {name:<6}  fmt={fmt}  rows={len(df_imu)}\")\n",
        "\n",
        "# Display best\n",
        "if results:\n",
        "    best_score, best_start, best_name, best_fmt, best_headers, best_df = results[0]\n",
        "    print(f\"\\n‚úÖ BEST IMU ‚Üí score={best_score:.3f}  start=+{best_start}  {best_name}  fmt={best_fmt}\")\n",
        "    display(best_df.head(25))\n",
        "\n",
        "    # Quick plots if we have accel/gyro (scaled)\n",
        "    cols_a = [c for c in [\"ax\",\"ay\",\"az\",\"ax_i16\",\"ay_i16\",\"az_i16\"] if c in best_df.columns]\n",
        "    cols_g = [c for c in [\"gx\",\"gy\",\"gz\",\"gx_i16\",\"gy_i16\",\"gz_i16\"] if c in best_df.columns]\n",
        "\n",
        "    if len(cols_a) >= 3:\n",
        "        plt.figure(figsize=(6,3))\n",
        "        for c in cols_a[:3]:\n",
        "            plt.plot(pd.to_numeric(best_df[c], errors=\"coerce\").values[:3000], alpha=0.8, label=c)\n",
        "        plt.title(\"Accel (first 3000 samples)\"); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "    if len(cols_g) >= 3:\n",
        "        plt.figure(figsize=(6,3))\n",
        "        for c in cols_g[:3]:\n",
        "            plt.plot(pd.to_numeric(best_df[c], errors=\"coerce\").values[:3000], alpha=0.8, label=c)\n",
        "        plt.title(\"Gyro (first 3000 samples)\"); plt.legend(); plt.tight_layout(); plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No plausible IMU layout found yet. We can extend the search window/variants.\")\n"
      ],
      "metadata": {
        "id": "iVjTnsE1NEtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AohRB1IrNEjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Decode all ASCII characters (skip binary masking)\n",
        "ascii_text = ''.join(chr(b) if 32 <= b < 127 else '.' for b in data)\n",
        "\n",
        "# Find all GNRMC sentences\n",
        "matches = list(re.finditer(r'\\$GNRMC,[^$]{20,100}', ascii_text))\n",
        "print(f\"Found {len(matches)} GNRMC sentences\")\n",
        "\n",
        "# Pick the first one for inspection\n",
        "if matches:\n",
        "    s = matches[0].group(0)\n",
        "    print(\"\\nüîπ Raw GNRMC sentence:\\n\", s)\n",
        "\n",
        "    # Split into fields\n",
        "    parts = s.split(',')\n",
        "    print(\"\\nüîπ Split fields:\")\n",
        "    for i, p in enumerate(parts):\n",
        "        print(f\"{i:02d}: {p}\")\n"
      ],
      "metadata": {
        "id": "i2HVhHxEQbBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = []\n",
        "for m in re.finditer(r'\\$GNRMC,[^$]{20,100}', ascii_text):\n",
        "    s = m.group(0)\n",
        "    if \",A,\" in s:\n",
        "        valid.append(s)\n",
        "\n",
        "print(f\"Found {len(valid)} valid (A) GNRMC sentences\")\n",
        "\n",
        "if valid:\n",
        "    print(\"\\nüîπ First valid fix:\\n\", valid[0])\n",
        "    print(\"\\nüîπ Split fields:\")\n",
        "    parts = valid[0].split(',')\n",
        "    for i, p in enumerate(parts):\n",
        "        print(f\"{i:02d}: {p}\")\n"
      ],
      "metadata": {
        "id": "TdVF1gakQmpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/input/20250907-150827_Rtk.fmnav\""
      ],
      "metadata": {
        "id": "cPeERqsESdtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =====================================================\n",
        "# Step ‚Äî Auto-detect schema, scale and visualize RTK data\n",
        "# =====================================================\n",
        "import folium\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df_best = pd.read_csv(\"/content/x200go_gnss_decoded.csv\")\n",
        "\n",
        "# Ensure df_best is loaded from your decoding step\n",
        "try:\n",
        "    df_best\n",
        "except NameError:\n",
        "    raise ValueError(\"‚ùå 'df_best' not found. Run the binary decoding cell first.\")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 1Ô∏è‚É£ Auto-detect schema\n",
        "# -----------------------------------------------------\n",
        "col_count = df_best.shape[1]\n",
        "\n",
        "# Define flexible name sets depending on decoded column count\n",
        "schema_map = {\n",
        "    8:  [\"timestamp\", \"lat_i\", \"lon_i\", \"alt_i\", \"v1\", \"v2\", \"v3\", \"q\"],\n",
        "    9:  [\"timestamp\", \"lat_i\", \"lon_i\", \"alt_i\", \"v1\", \"v2\", \"v3\", \"extra\", \"q\"],\n",
        "    10: [\"timestamp\", \"lat_i\", \"lon_i\", \"alt_i\", \"v1\", \"v2\", \"v3\", \"extra1\", \"extra2\", \"q\"],\n",
        "    11: [\"timestamp\", \"lat_i\", \"lon_i\", \"alt_i\", \"v1\", \"v2\", \"v3\", \"ax\", \"ay\", \"az\", \"q\"]\n",
        "}\n",
        "\n",
        "if col_count not in schema_map:\n",
        "    raise ValueError(f\"Unexpected number of columns ({col_count}). Please check the decoding step.\")\n",
        "\n",
        "df_best.columns = schema_map[col_count]\n",
        "\n",
        "print(f\"‚úÖ Assigned column names for {col_count} columns: {df_best.columns.tolist()}\")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 2Ô∏è‚É£ Convert integer fields to real-world units\n",
        "# -----------------------------------------------------\n",
        "df_best[\"lat_deg\"] = df_best[\"lat_i\"] * 1e-7\n",
        "df_best[\"lon_deg\"] = df_best[\"lon_i\"] * 1e-7\n",
        "df_best[\"alt_m\"]   = df_best[\"alt_i\"] * 0.001  # if in millimeters\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 3Ô∏è‚É£ Clean and filter\n",
        "# -----------------------------------------------------\n",
        "df_best = df_best.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"lat_deg\", \"lon_deg\"])\n",
        "df_best = df_best[df_best[\"lat_deg\"].between(-90, 90)]\n",
        "df_best = df_best[df_best[\"lon_deg\"].between(-180, 180)]\n",
        "df_best = df_best[df_best[\"lat_deg\"].diff().abs() + df_best[\"lon_deg\"].diff().abs() > 0]\n",
        "\n",
        "print(f\"‚úÖ {len(df_best)} valid records after filtering.\")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 4Ô∏è‚É£ Summary\n",
        "# -----------------------------------------------------\n",
        "if not df_best.empty:\n",
        "    print(f\"lat_deg: min={df_best['lat_deg'].min():.6f}, med={df_best['lat_deg'].median():.6f}, max={df_best['lat_deg'].max():.6f}\")\n",
        "    print(f\"lon_deg: min={df_best['lon_deg'].min():.6f}, med={df_best['lon_deg'].median():.6f}, max={df_best['lon_deg'].max():.6f}\")\n",
        "    print(f\"alt_m:   min={df_best['alt_m'].min():.3f}, med={df_best['alt_m'].median():.3f}, max={df_best['alt_m'].max():.3f}\")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 5Ô∏è‚É£ Visualize trajectory\n",
        "# -----------------------------------------------------\n",
        "if len(df_best) > 0:\n",
        "    avg_lat = df_best[\"lat_deg\"].mean()\n",
        "    avg_lon = df_best[\"lon_deg\"].mean()\n",
        "\n",
        "    m = folium.Map(location=[avg_lat, avg_lon], zoom_start=15, tiles=\"OpenStreetMap\")\n",
        "\n",
        "    folium.PolyLine(\n",
        "        df_best[[\"lat_deg\", \"lon_deg\"]].values.tolist(),\n",
        "        color=\"blue\", weight=2, opacity=0.8\n",
        "    ).add_to(m)\n",
        "\n",
        "    folium.Marker(\n",
        "        [df_best.iloc[0][\"lat_deg\"], df_best.iloc[0][\"lon_deg\"]],\n",
        "        popup=\"Start\", icon=folium.Icon(color=\"green\")\n",
        "    ).add_to(m)\n",
        "    folium.Marker(\n",
        "        [df_best.iloc[-1][\"lat_deg\"], df_best.iloc[-1][\"lon_deg\"]],\n",
        "        popup=\"End\", icon=folium.Icon(color=\"red\")\n",
        "    ).add_to(m)\n",
        "\n",
        "    print(\"\\nüó∫Ô∏è Interactive trajectory map:\")\n",
        "    display(m)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No valid coordinates to visualize.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sz3JXsyNRPqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Search header for possible base coordinates\n",
        "# =====================================================\n",
        "with open(\"/content/x200go_gnss_decoded.csv\", \"rb\") as f:\n",
        "    head = f.read(2048)\n",
        "\n",
        "import re\n",
        "ascii_text = ''.join(chr(b) if 32 <= b < 127 else '.' for b in head)\n",
        "\n",
        "# Look for degree-like numeric patterns (e.g., 24.xxxxxx or 67.xxxxxx)\n",
        "matches = re.findall(r\"\\d{2}\\.\\d{4,6}\", ascii_text)\n",
        "print(\"üîç Possible coordinate-like patterns in header:\")\n",
        "for m in matches:\n",
        "    print(\"  \", m)\n"
      ],
      "metadata": {
        "id": "bRkXZbW7UYm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Extract and plot all coordinate-like values on Folium map\n",
        "# =====================================================\n",
        "import re\n",
        "import folium\n",
        "import numpy as np\n",
        "\n",
        "file_path = \"/content/x200go_gnss_decoded.csv\"\n",
        "\n",
        "# 1Ô∏è‚É£ Read binary and convert to ASCII-like string\n",
        "with open(file_path, \"rb\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "ascii_data = ''.join(chr(b) if 32 <= b < 127 else '.' for b in data)\n",
        "\n",
        "# 2Ô∏è‚É£ Extract all numeric-like substrings that look like coordinates\n",
        "matches = re.findall(r\"\\d{2}\\.\\d{3,6}\", ascii_data)\n",
        "values = [float(v) for v in matches]\n",
        "\n",
        "# 3Ô∏è‚É£ Separate plausible latitude/longitude ranges\n",
        "lat_vals = [v for v in values if 0 < v < 60]     # latitudes\n",
        "lon_vals = [v for v in values if 60 <= v < 180]  # longitudes\n",
        "\n",
        "print(f\"üìÑ Extracted {len(values)} numeric-like values\")\n",
        "print(f\"   ‚Üí {len(lat_vals)} possible latitudes\")\n",
        "print(f\"   ‚Üí {len(lon_vals)} possible longitudes\")\n",
        "\n",
        "# 4Ô∏è‚É£ Pair roughly by index\n",
        "pairs = list(zip(lat_vals[:len(lon_vals)], lon_vals[:len(lat_vals)]))\n",
        "\n",
        "if not pairs:\n",
        "    raise ValueError(\"‚ö†Ô∏è No coordinate-like pairs found!\")\n",
        "\n",
        "# 5Ô∏è‚É£ Compute map center\n",
        "avg_lat = np.mean([p[0] for p in pairs])\n",
        "avg_lon = np.mean([p[1] for p in pairs])\n",
        "\n",
        "print(f\"üó∫Ô∏è Centering map around: lat={avg_lat:.6f}, lon={avg_lon:.6f}\")\n",
        "print(f\"   Showing {len(pairs)} coordinate candidates\")\n",
        "\n",
        "# 6Ô∏è‚É£ Create Folium map\n",
        "m = folium.Map(location=[avg_lat, avg_lon], zoom_start=6, tiles=\"OpenStreetMap\")\n",
        "\n",
        "# Add all points\n",
        "for lat, lon in pairs:\n",
        "    folium.CircleMarker(\n",
        "        location=[lat, lon],\n",
        "        radius=3,\n",
        "        color=\"blue\",\n",
        "        fill=True,\n",
        "        fill_opacity=0.6\n",
        "    ).add_to(m)\n",
        "\n",
        "# Highlight approximate cluster center\n",
        "folium.Marker(\n",
        "    [avg_lat, avg_lon],\n",
        "    popup=f\"Cluster Center\\n({avg_lat:.6f}, {avg_lon:.6f})\",\n",
        "    icon=folium.Icon(color=\"red\", icon=\"info-sign\")\n",
        ").add_to(m)\n",
        "\n",
        "display(m)\n"
      ],
      "metadata": {
        "id": "ZjNv28AGVajx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}